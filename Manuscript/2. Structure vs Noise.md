## Structure vs Noise

The basic intuition behind the dynamics of the neuron, as it experiences the changing input environment, comes from an understanding of the competition between *structure* and *noise*.  These terms refer to the statistics of the input environment and the mathematical form of the plasticity equations -- different sets of equations will respond to different forms of *structure* and have different dynamics under *noise*.  This can be used to compare different theories of synaptic plasticity[@BlaisEtAl99] but is beyond what we want to discuss here.

In the case of the BCM theory, the *structure* comes in the form of an environment with a minority of patterns yielding high responses and the majority yielding low responses -- the BCM neuron can become selective in this environment.  *Noise* by definition does not have this property.  It is either random variation on top of the structure or random input without the patterns that can lead to high responses.   Gaussian (i.e. normal) noise is an example of such variation.  Examples of structure and noise can be visually seen in [Figure @fig:structure_vs_noise].

![[Pasted image 20240117155651.png]]
> Structure (left) and noise (right) for a BCM neuron. Shown on the axes are the input values for a two-input neuron, the value of input $x_1$ shown on the x-axis and input $x_2$ on the y-axis.  Each tiny blue dot is a snapshot of the activity of $(x_1,x_2)$ at some time.  Over time, as the neuron experiences the environment, the activity plot fills in the cloud shown.  In the case of structure can see a minority input values that are high, extending out in the x- and y-directions compared to the noise (right figure) where all directions are equal.   In the structured environment, the weights for the BCM neuron converge to those directions such that the postsynaptic response is selective.{#fig:structure_vs_noise}


The *structure* in this case is generated using a distribution known as the Laplace distribution, or double exponential
$$
P(x) \sim e^{-|x|}
$$

The *noise* is generated using a Gaussian or Normal distribution,
$$
P(x) \sim e^{-x^2}
$$
Mathematically the difference leads to the Laplace environment having more values near zero and a significant number of patterns at very high values (compared to the Gaussian) -- rare patterns.  This property of the Laplace distribution (and some other distributions) is sometimes referred to as having "heavy tails".   This mathematical property is exactly the property the BCM neuron selects for when it learns, so we expect the BCM to become selective in an environment produced by a Laplace distribution vs a Gaussian one.  We can see the property of rare input patterns by looking at the distribution itself as in [Figure @ @fig:laplace_dist], shown on a semi-log plot do highlight those high-value patterns.   Compared to a Gaussian, the Laplace distribution has many more patterns with significantly higher values.  The natural image environment also has a similar structure to the Laplace environment, even for individual pixels as also shown in [Figure @ @fig:laplace_dist].  


![[Pasted image 20240228124006.png]]
> Single input (pixel) statistics for the Laplace environment and the natural image environment compared to Gaussian (i.e. Normal) noise. {#fig:laplace_dist}

As the BCM neuron learns, it starts with an output distribution more similar to a Gaussian and learns to respond more to the rare patterns with high activity, shown in [Figure @fig:2D_BCM_Laplace] for a 2D Laplace environment and [Figure @fig:BCM_Natural_Image] for a natural image environment.  The rare patterns are the ones above the BCM modification threshold, $\theta_M$. 

![[Pasted image 20240228124702.png]]
> Output distributions for a BCM neuron in a 2D Laplace environment.  Shown are (top) the distribution of responses before any learning and (bottom) after learning compared to a Gaussian distribution of responses with the same standard deviation.  In both cases, the responses are smeared across the entire range, with fewer patterns yielding high responses.  Because these plots are on a log scale, Gaussian responses appear curved down as activity increases while Laplace responses appear as a straight decline.  The BCM neuron finds directions in the input space such that its responses are more Laplacian -- they respond more to a smaller group of patterns.{#fig:2D_BCM_Laplace}


![[Pasted image 20240228124932.png]]
> Output distributions for a BCM neuron in a natural image environment.  Shown are (top) the distribution of responses before any learning and (bottom) after learning compared to a Gaussian distribution of responses with the same standard deviation.  In both cases, the responses are smeared across the entire range, with fewer patterns yielding high responses.  Because these plots are on a log scale, Gaussian responses appear curved down as activity increases while Laplace responses appear as a straight decline.  The BCM neuron finds directions in the input space such that its responses are more Laplacian -- they respond more to a smaller group of patterns.{#fig:BCM_Natural_Image}



