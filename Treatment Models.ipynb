{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09462893-17fe-4c70-9f18-ed69266353b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macOS-10.16-x86_64-i386-64bit\n",
      "Base Image File: asdf/bbsk081604_all_log2dog.asdf\n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plasticnet as pn\n",
    "import process_images_hdf5 as pi5\n",
    "from deficit_defs import patch_treatment\n",
    "\n",
    "from matplotlib.pyplot import figure,xlabel,ylabel,legend,gca,plot,subplot,imshow,axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64851642-1dd0-4954-be48-b5afb99ef328",
   "metadata": {},
   "source": [
    "## Models of Treatments for Amblyopia {#sec-models-of-treatments}\n",
    "\n",
    "To model the fix to the refractive imbalance we follow the deficit simulation with an input environment that is rebalanced, both eyes receiving nearly identical input patches (@fig-normal-inputs).   This process is a model of the application of refractive correction.  Although both eyes receive nearly identical input patches, we add independent Gaussian noise to each input channel to represent the natural variation in the activity in each eye.  In addition, in those cases where use employ strabismic amblyopia, the inter-eye jitter is not corrected with the refractive correction.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "292f4f61-5507-4a27-ae00-c1f9632778b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache_images_de75a60123a7c262a30ea675be766aff.asdf from cache.[Grating time elapsed 5.81 s\n",
      ".] Sequence Time Elapsed...7.17 s\n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "\n",
    "def inputs_to_images(X,buffer=5):\n",
    "    ims=[]\n",
    "    vmin=X.min()\n",
    "    vmax=X.max()\n",
    "    \n",
    "    rf_size=int(np.sqrt(X.shape[1]/2))\n",
    "    \n",
    "    for xx in X:\n",
    "        xx1=xx[:rf_size*rf_size].reshape(rf_size,rf_size)\n",
    "        xx2=xx[rf_size*rf_size:].reshape(rf_size,rf_size)\n",
    "        im=np.concatenate((xx1,np.ones((rf_size,buffer))*vmax,xx2),axis=1)   \n",
    "        ims.append(im)\n",
    "        \n",
    "    return ims\n",
    "\n",
    "def get_input_patch_examples_treatment():\n",
    "    \n",
    "    seq=pn.Sequence()    \n",
    "    seq+=patch_treatment(patch_noise=0.5,\n",
    "               total_time=1000,number_of_neurons=1,\n",
    "               eta=1e-6,\n",
    "               save_interval=1)\n",
    "    sim=seq.sims[0]\n",
    "    pre=seq.neurons[0][0][0]\n",
    "    sim.monitor(pre,['output'],1)\n",
    "\n",
    "    seq.run(display_hash=False,print_time=True)\n",
    "    m=sim.monitors['output']\n",
    "    t,X=m.arrays()    \n",
    "    \n",
    "    return sim,X\n",
    "\n",
    "sim,X=get_input_patch_examples_treatment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b3103b6-5b1e-49fb-bcca-dff5e57cb6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache_images_de75a60123a7c262a30ea675be766aff.asdf from cache."
     ]
    }
   ],
   "source": [
    "    seq=pn.Sequence()    \n",
    "    seq+=patch_treatment(patch_noise=0.5,\n",
    "               total_time=100,number_of_neurons=1,\n",
    "               eta=1e-6,\n",
    "               save_interval=1)\n",
    "    sim=seq.sims[0]\n",
    "    pre=seq.neurons[0][0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a349091-8d6b-477e-8442-1ea8f4ff24a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<plasticnet.neurons.pattern_neuron.natural_images at 0x7fa070153e50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.neurons[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "434a8746-390c-4b78-a49d-cb95d8f70908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.        , -0.        , -0.        , ..., -0.        ,\n",
       "       -0.        ,  0.04363927])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a476652-2325-4ce5-88a6-3b1cc6263893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache_images_de75a60123a7c262a30ea675be766aff.asdf from cache.[Grating time elapsed 0.56 s\n",
      ".] Sequence Time Elapsed...1.87 s\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| label: fig-patch-inputs\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#| fig-cap: A sample of 24 input patches from a patched visual environment. \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#| \u001b[39;00m\n\u001b[1;32m      4\u001b[0m sim,X\u001b[38;5;241m=\u001b[39mget_input_patch_examples_treatment()\n\u001b[0;32m----> 5\u001b[0m ims\u001b[38;5;241m=\u001b[39m\u001b[43minputs_to_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m figure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m24\u001b[39m):\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36minputs_to_images\u001b[0;34m(X, buffer)\u001b[0m\n\u001b[1;32m      5\u001b[0m vmin\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mmin()\n\u001b[1;32m      6\u001b[0m vmax\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m----> 8\u001b[0m rf_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xx \u001b[38;5;129;01min\u001b[39;00m X:\n\u001b[1;32m     11\u001b[0m     xx1\u001b[38;5;241m=\u001b[39mxx[:rf_size\u001b[38;5;241m*\u001b[39mrf_size]\u001b[38;5;241m.\u001b[39mreshape(rf_size,rf_size)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "#| label: fig-patch-inputs\n",
    "#| fig-cap: A sample of 24 input patches from a patched visual environment. \n",
    "#| \n",
    "sim,X=get_input_patch_examples_treatment()\n",
    "ims=inputs_to_images(X,buffer=2)\n",
    "figure(figsize=(20,6))\n",
    "for i in range(24):\n",
    "    im=ims[i]\n",
    "    subplot(4,6,i+1)\n",
    "    imshow(im,cmap=plt.cm.gray)\n",
    "    axis('off')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "416d1e21-94bd-4ff0-a6c0-060dca4ed3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=sim.monitors['output']\n",
    "t,X=m.arrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d678e5-4258-46d1-9628-1b8946f9eb81",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Patch treatment\n",
    "\n",
    "The typical patch treatment is done by depriving the strong-eye of input with an eye-patch.  In the model this is equivalent to presenting the strong-eye with random noise instead of the natural image input.  Competition between the left- and right-channels drives the recovery, and is produced from the difference between *structured* input into the weak-eye and the *unstructured* (i.e. noise) input into the strong eye.  It is not driven by a reduction in input activity.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3101f-f582-40bc-9860-5f5120d8f80a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Contrast modification\n",
    "\n",
    "A binocular approach to treatment can be produced with contrast reduction of the non-deprived channel relative to the deprived channel. Experimentally this can be accomplished with VR headsets[@xiao2020improved]. In the model we implement this by down-scaling the normal, unblurred channel with a simple scalar multiplier applied to each pixel (Figure [4](#fig:input) D). The contrast difference sets up competition between the two channels with the advantage given to the weak-eye channel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b3185-cd41-4c9f-95f8-ef1970de1c52",
   "metadata": {},
   "source": [
    "## Dichoptic Mask\n",
    "\n",
    "On top of the contrast modification, we can include the application of the dichoptic mask (Figure @fig:input E).  In this method, each eye receives a version of the input images filtered through independent masks in each channel, resulting in a mostly-independent pattern in each channel.  \n",
    "It has been observed that contrast modification combined with dichoptic masks can be an effective treatment for amblyopia[@Li:2015aa,@xiao2021randomized].  The motivation behind the application of the mask filter is that the neural system must use both channels to reconstruct the full image and thus may lead to enhanced recovery.  \n",
    "\n",
    "The dichoptic masks are constructed with the following procedure.  A blank image (i.e. all zeros) is made to which is added 15 randomly sized circles with values equal to 1 (Figure @fig:dichopic_blob).   These images are then smoothed with a Gaussian filter of a given width, $f$.  This width is a parameter we can vary to change the overlap between the left- and right-eye images.  A high value of $f$ compared with the size of the receptive field, e.g. $f=90$, yields a high overlap between the patterns in the weak- and strong-eye inputs (Figure @fig:dichopic_filter_size).  Likewise, a small value of $f$, e.g. $f=10$, the eye inputs are nearly independent -- the patterned activity falling mostly on one of the eyes and not much to both.  Finally, the smoothed images are scaled to have values from a minimum of 0 to a maximum of 1.  This image-mask we will call $A$, and is the left-eye mask whereas the right-eye mask, $F$, is the inverse of the left-eye mask, $F\\equiv 1-A$.  The mask is applied to an image by multiplying the left- and right-eye images by the left- and right-eye masks, respectively, resulting in a pair of images which have no overlap at the peaks of each mask, and nearly equal overlap in the areas of the images where the masks are near 0.5 (Figure @fig:dichopic_filter_image).   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cecc2de-6442-4d21-8fef-307e7667ac12",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Atropine treatment\n",
    "\n",
    "In the atropine treatment for amblyopia[@glaser2002randomized], eye-drops of atropine are applied to the strong-eye resulting in blurred vision in that eye.  Here we use the same blurred filter used to obtain the deficit (possibly with a different width) applied to the strong eye (Figure @fig:input F).  The difference in sharpness between the strong-eye inputs and the weak-eye inputs sets up competition between the two channels with the advantage given to the weak-eye.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69436e4-6307-4ba2-89e2-d6ca86002cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
